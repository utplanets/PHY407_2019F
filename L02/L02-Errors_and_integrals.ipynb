{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Author: Nico Grisouard, nicolas.grisouard@physics.utoronto.ca**\n",
    "\n",
    "**Modified: Christopher Lee, clee@atmosp.physics.utoronto.ca**\n",
    "\n",
    "*Supporting textbook chapters for week 2: 4.3, 5.1-5.3.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This week's topics:\n",
    "* Numerical errors,\n",
    "* How to compute integrals (basic methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical errors\n",
    "\n",
    "Aside from errors in programming or discretizing the physical model, there are 2 types of errors in computations:\n",
    "1. Rounding errors: errors in how the computer stores or manipulates numbers.\n",
    "2. Approximation errors (sometimes called truncation errors): errors in approximations to various functions or methods.\n",
    "\n",
    "Reason: computers are machines, with inherent limitations.\n",
    "\n",
    "Which begs the question: how does a computer represent a number?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Preliminary step: integers and floats\n",
    "\n",
    "* Variables `a`, `b` and `c` below are all equal to 1000.\n",
    "* Yet, `a` is treated very differently by da python than `b` and `c`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = 1000\n",
    "b = 1000.\n",
    "c = 1e3 \n",
    "print('The type of a is', type(a))\n",
    "print('The type of b is', type(b))\n",
    "print('The type of c is', type(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error #1: rounding errors:\n",
    "\n",
    "### General principle\n",
    "\n",
    "* Integer numbers: python does not limit the number of digits stored. Can cause problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1267650600228229401496703205376\n"
     ]
    }
   ],
   "source": [
    "print(2**100)  # increase the exponent from 10 to 10,000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.int(2**63-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Non-integer numbers with more than 16 significant figures: **rounding** after 16 figures.\n",
    "\n",
    "* Each mathematical operation (FLOPS, Floating-point Operations Per Second) introduces errors in the 16th digit. You can’t assume `1.3+2.4=3.7`, it might be `3.699999999999999` even though the 2 numbers you are adding only have 2 significant figures.\n",
    "\n",
    "* Know this, accept it, work with it. You are better at adding 1.3 and 2.4 than your computer, but you are much slower. This is why we use computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are also limitations as to the largest number representable in python, the closest to zero, etc.\n",
    "If you want to know what they are on your machine, you can run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributes you can access in finfo(float64)  ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_finfo_cache', '_init', '_str_eps', '_str_epsneg', '_str_max', '_str_resolution', '_str_tiny', 'bits', 'dtype', 'eps', 'epsneg', 'iexp', 'machar', 'machep', 'max', 'maxexp', 'min', 'minexp', 'negep', 'nexp', 'nmant', 'precision', 'resolution', 'tiny']\n",
      "maximum numbers in 64 bit and 32 bit precision:  1.7976931348623157e+308 3.4028235e+38\n",
      "minimum numbers in 64 bit and 32 bit precision:  -1.7976931348623157e+308 -3.4028235e+38\n",
      "epsilon for 64 bit and 32 bit:  2.220446049250313e-16 1.1920929e-07\n",
      "Should be epsilon for this machine if it's 64 bit 2.220446049250313e-16\n",
      "Should be zero 0.0\n"
     ]
    }
   ],
   "source": [
    "from numpy import finfo, float64, float32\n",
    "# float64 contains double-precision floats, float32 is single-precision\n",
    "print(\"attributes you can access in finfo(float64) \", dir(finfo(float64)))\n",
    "print( \"maximum numbers in 64 bit and 32 bit precision: \",\n",
    "      finfo(float64).max, finfo(float32).max)\n",
    "print( \"minimum numbers in 64 bit and 32 bit precision: \",\n",
    "      finfo(float64).min, finfo(float32).min)\n",
    "print( \"epsilon for 64 bit and 32 bit: \",\n",
    "      finfo(float64).eps, finfo(float32).eps)\n",
    "print( \"Should be epsilon for this machine if it's 64 bit\",\n",
    "      float64(1)+finfo(float64).eps-float64(1))\n",
    "print( \"Should be zero\",\n",
    "      float64(1)+finfo(float64).eps/2.0-float64(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finfo(float64).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A dangerous example\n",
    "\n",
    "Remember this one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x+y) + z =  1.0\n",
      "x + (y+z) =  0.0\n"
     ]
    }
   ],
   "source": [
    "x = 1e20\n",
    "y = -1e20\n",
    "z = 1.\n",
    "print(\"(x+y) + z = \", (x+y)+z)\n",
    "print(\"x + (y+z) = \", x+z+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how about this one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7./3. - 4./3 - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Error constant\n",
    "\n",
    "* Newman: $\\sigma = C |x|$.\n",
    "    * $x =$ number you want to represent.\n",
    "    * $\\sigma =$ standard deviation of error \n",
    "    * $C =$ fractional error for a single floating point number\n",
    "* For 64 bit float: $C \\sim 10^{-16} \\sim$ machine precision $\\epsilon_M$.\n",
    "* We cannot know a number better than this on the computer (otherwise it wouldn't be a limit on the precision).\n",
    "* This fractional error is different on different computers but should not depend on $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Propagation of errors\n",
    "\n",
    "* Errors propagate statistically like they do in experimental physics.\n",
    "* Example that follows: let $C = 1\\%$ (to exagerate the effects) and add two numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# example to illustrate machine error using large error constant C\n",
    "# to make it easier to see graphically\n",
    "\n",
    "from numpy.random import normal  # import normal distribution\n",
    "from numpy import arange\n",
    "# import plotting functions below:\n",
    "from pylab import hist, show, subplot, figure, xlabel, xticks, legend, savefig\n",
    "from matplotlib import rcParams  # import rcparams to change font size\n",
    "rcParams.update({'font.size':14, 'legend.fontsize':10})\n",
    "\n",
    "# define array size\n",
    "N = 1_000_000\n",
    "# define number of bins for histogram\n",
    "N_Bins = 100\n",
    "# define C, which is our simulated error constant\n",
    "C = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define numbers\n",
    "(x1, x2) = (3, -3.2)\n",
    "# define error standard deviations in terms of C\n",
    "sigma1 = C*abs(x1)\n",
    "sigma2 = C*abs(x2)\n",
    "# define distributions to those numbers satisfying sigma = Cx\n",
    "# This is how we simulate error.\n",
    "d1 = normal(loc=x1, scale=sigma1, size=N)\n",
    "d2 = normal(loc=x2, scale=sigma2, size=N)\n",
    "\n",
    "# then add up the distributions\n",
    "# then calculate the distribution of the sums.\n",
    "sumd = d1 + d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# this plot just shows the a single number and its error\n",
    "figure(2, figsize=((7, 7)))\n",
    "hist(d1, N_Bins, histtype='stepfilled')\n",
    "xlabel('mean +- absolute error:' + str(x1) + ' +- ' + str(sigma1))\n",
    "savefig('MachineError_x1=' + str(x1) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "figure(1, figsize=((7, 12)))\n",
    "# plot histograms of the two numbers and their sum\n",
    "ax = subplot(2, 1, 1)\n",
    "hist(d1, N_Bins, histtype='stepfilled')\n",
    "hist(sumd, N_Bins,  histtype='stepfilled')\n",
    "hist(d2, N_Bins, histtype='stepfilled')\n",
    "leg=legend((str(x1), 'sum', str(x2)), loc='upper left')\n",
    "ax.grid(True)\n",
    "xlabel('mean + absolute error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# then plot fractional error by diving by the mean values.\n",
    "# fractional errors will be large for opposite-signed large numbers\n",
    "# offset and spread have to be adjusted to make the plots clear.\n",
    "ax = subplot(2, 1, 2)\n",
    "offset = 2\n",
    "spread = offset/4.0\n",
    "hist(-offset+(d1-x1)/x1, N_Bins, histtype='stepfilled')\n",
    "\n",
    "hist((sumd-(x1+x2))/(x1+x2), N_Bins, histtype='stepfilled')\n",
    "hist(offset+(d2-x2)/x2, N_Bins, histtype='stepfilled')\n",
    "\n",
    "xticks([-offset-spread, -offset, -offset+spread, -spread, 0,\n",
    "        spread, offset-spread, offset, offset+spread],\n",
    "       [str(-spread), str(0.0), str(spread), str(-spread),\n",
    "        str(0.0), str(spread), str(-spread), str(0.0), str(spread)])\n",
    "\n",
    "ax.grid(True)\n",
    "\n",
    "xlabel('fractional error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These errors in differences become **very important** when taking numerical derivatives:\n",
    "$$\\frac{df}{dt} \\approx \\frac{f_{i+1} - f_i}{\\Delta t} = \\text{danger zone}$$\n",
    "as we will see next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### One important rule\n",
    "\n",
    "Never, ever. Never ever ever. Do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "if 7./3. - 4./3. - 1. == 0:\n",
    "    print('7/3 - 4/3 - 1 = 0')\n",
    "else:\n",
    "    print('7/3 - 4/3 - 1 is not equal to 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "delta = 1e-15\n",
    "if abs(7./3. - 4./3. - 1.) < delta:\n",
    "    print('7/3 - 4/3 - 1 = 0')\n",
    "else:\n",
    "    print('7/3 - 4/3 - 1 is not equal to 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error #2: Approximation errors\n",
    "\n",
    "* errors introduced in functions due to approximations\n",
    "* very important to consider for **integration** & **differentiation** algorithms\n",
    "* we approximate these operations and there is an error in that approximation\n",
    "* Most integration/differentiation algorithms are somehow based on Taylor series expansions, so you can usually figure out the approximation error by looking at the terms in the Taylor expansion that you ignore.\n",
    "\n",
    "This should become clearer once we illustrate it with numerical integrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical integration\n",
    "\n",
    "* Think of integrals as areas under curves.\n",
    "* Approximate these areas in terms of simple shapes (rectangles, trapezoids, rectangles with parabolic tops)\n",
    "\n",
    "![From Newman, composite of figs. 5.1 and 5.2.](RecTrapSimp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Trapezoidal rule\n",
    "\n",
    "* Break up internal into $N$ slices,\n",
    "* Approximate function as segments on each slice.\n",
    "\n",
    "\n",
    "![From Newman: fig. 5.1b](Fig5-1b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* N slices from a to b means that slice width: $$h = (b – a )/N$$\n",
    "* area of ‘k’ slice’s trapezoid: (Rectangle + Triangle)\n",
    "$$A_k = f(x_k)h + \\frac{h[f(x_k+h)-f(x_k)]}2 = \\frac{h[f(x_k) + f(x_k +h)]}2.$$\n",
    "* Total area (our approximation for the integral) (and using $x_k=a+kh$):\n",
    "$$ \\boxed{I(a, b) = h\\left[\\frac12 f(a) + \\frac12f(b) + \\sum_{k=1}^{N-1} f(a+kh)\\right].} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\text{Recall } I(a, b) = h\\left[\\frac12 f(a) + \\frac12f(b) + \\sum_{k=1}^{N-1} f(a+kh)\\right].$$\n",
    "\n",
    "For future reference, let's do a symbolic integral of $f(x) = x^4 - 2x + 1$ on the interval $[0, 2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy  # the symbolic math package\n",
    "sympy.init_printing()\n",
    "xs = sympy.Symbol('xs', real=True)  # the variable of integration\n",
    "sympy.integrate(xs**4 - 2*xs + 1, (xs,0,2.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\text{Recall } I(a, b) = h\\left[\\frac12 f(a) + \\frac12f(b) + \\sum_{k=1}^{N-1} f(a+kh)\\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Newman's example on pp. 142-143\n",
    "def f(x):\n",
    "    return x**4 - 2*x + 1\n",
    "\n",
    "N = 100  # number of slices; try and increase it\n",
    "a = 0.0  # beginning of interval\n",
    "b = 2.0  # end of interval\n",
    "h = (b-a)/N  # width of slice\n",
    "\n",
    "s = 0.5*f(a) + 0.5*f(b)  # the end bits\n",
    "for k in range(1,N):  # adding the interior bits\n",
    "    s += f(a+k*h)  # and \n",
    "\n",
    "print(h*s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Simpson's rule\n",
    "\n",
    "* Break up internal into $N$ slices,\n",
    "* approximate function as a **quadratic** for every 2 slices\n",
    "* need 2 slices because you need 3 points to define a quadratic\n",
    "* more slices $\\Rightarrow$ better approximation to function\n",
    "* Number of slices need to be even! If uneven, either discard one, or use trapezoidal rule on one slice.\n",
    "\n",
    "![From Newman: fig. 5.2](Fig5-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Area of each 2-slice quadratic (see text for formula):\n",
    "$$A_k = \\frac{h}3\\left[f(a+(2k-2)h) + 4f(a+(2k-1)h) + f(a+2kh)\\right].$$\n",
    "* Adding up the slices:\n",
    "$$\\boxed{I(a,b) = \\frac{h}3\\left[f(a) + f(b) + 4\\sum_{\\substack{k\\ odd\\\\ 1\\dots{}N-1}}f(a+kh) + 2\\sum_{\\substack{k\\ even \\\\ 2\\dots{}N-2}}f(a+kh)\\right].}$$\n",
    "* In python, you can easily sum over even and odd values:\n",
    "\n",
    "`for k in range(1, N, 2)` for the odd terms, and\n",
    "\n",
    "`for k in range(2, N, 2)` for the even terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Error estimation\n",
    "\n",
    "* If you tried the trapezoidal integration routine, you noticed that the error (difference between true value of the integral and computed value) goes down as $N$ increases.\n",
    "* How fast?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Euler-MacLaurin formulas\n",
    "\n",
    "Example, for the trapezoidal rule:\n",
    "$$ I(a, b) = \\int_a^b f(x)dx = \\underbrace{h\\left[\\frac12 f(a) + \\frac12f(b) + \\sum_{k=1}^{N-1} f(a+kh)\\right]}_{\\text{the method}} + \\underbrace{\\epsilon}_{\\text{the error}}$$\n",
    "\n",
    "* using Taylor expansions, can find order of error for different Newton-Cotes (see text; trapeze and Simpson are examples) formulas.\n",
    "* These error formulas are called “Euler-MacLaurin” formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Trapezoidal rule is a \"1$^{\\text{st}}$-order\" integration rule, i.e. accurate up to and including terms proportional to $h$. Leading order approximation error is of order $h^2$:\n",
    "$$\\boxed{\\epsilon = \\frac{h^2[f'(a) - f'(b)]}{12} + h.o.t.}$$\n",
    "(see text for derivation)\n",
    "\n",
    "* Simpson’s rule is a \"3$^{\\text{rd}}$-order\" integration rule, i.e., accurate up to and including terms proportional to $h^3$. Leading order approximation error is of order $h^4$ (even though we go from segments to quadratics!)\n",
    "$$\\boxed{\\epsilon = \\frac{h^4[f'''(a) - f'''(b)]}{180} + h.o.t.}$$\n",
    "(see text for non-derivation, and typo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A more practical estimate\n",
    "\n",
    "* What if you don't know $f'$, $f'''$, etc.?\n",
    "* if you know the order of the error, (e.g., $\\epsilon \\propto h^2$ for trapeze), there is a way.\n",
    "\n",
    "1. choose $N$ intervals, compute $I_N(a, b)$. You **know** that $$I(a, b) = I_N(a, b) + Ch^2$$ (for trapz). But you don't know $C$.\n",
    "2. Double $N$: compute $I_{2N}(a, b)$. You **know** that $$\\underbrace{I(a, b)}_{true} = \\underbrace{I_{2N}(a, b)}_{computed} + \\underbrace{C\\left(\\frac{h}2\\right)^2}_{error} = I_{2N}(a, b) + \\frac{C}4 h^2.$$ $C$ does not change: e.g., for trapz, $C = [f'(a) - f'(b)]/12$.\n",
    "3. Both equations above are $= I(a, b)$, the \"true\" value. Substract and re-arrange:\n",
    "$$\\boxed{\\underbrace{\\frac{I_{2N}(a, b) - I_{N}(a, b)}{h^2}}_{known} = \\frac{3}{4} \\underbrace{C}_{unknown}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* For Simpson's rule, we would find:\n",
    "$$\\frac{I_{2N}(a, b) - I_{N}(a, b)}{h^4} = \\frac{16}{15}C.$$\n",
    "* Works in principle for any Newton-Cotes formula.\n",
    "* Basic idea:\n",
    "    1. compute integral from $N$ points,\n",
    "    2. double $N$, compute again,\n",
    "    3. compare.\n",
    "    \n",
    "* Note: the texbook uses a different convention, using $h/2$ as reference instead of my $h$, hence the different coefficients."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_metadata": {
   "affiliation": "PHY407, University of Toronto",
   "author": "Nico Grisouard",
   "title": "Lecture 1: python basics"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
